{"id":3310,"date":"2020-07-10T12:51:39","date_gmt":"2020-07-10T16:51:39","guid":{"rendered":"https:\/\/achievion.com\/?p=3310"},"modified":"2020-07-10T12:54:16","modified_gmt":"2020-07-10T16:54:16","slug":"how-ai-can-help-find-the-optimal-solution-through-iterations-of-trial-and-error","status":"publish","type":"post","link":"https:\/\/achievion.com\/blog\/how-ai-can-help-find-the-optimal-solution-through-iterations-of-trial-and-error.html","title":{"rendered":"How AI Can Help Find the Optimal Solution through Iterations of Trial And Error"},"content":{"rendered":"\n<p>This post is the sixth entry in our series on AI application patterns across multiple industries. In this post, we will discuss how AI and machine learning can help find the optimal solution through iterations of trial and error.<\/p>\n\n\n\n<h2>Understanding Reinforcement Learning: The Machine Learning Type Typically Associated With the Trial and Error Process<\/h2>\n\n\n\n<p>When discussing AI application patterns, a key pattern of AI that cannot be missed are goal-driven systems. We describe this as the use of machine learning and other approaches to cognition to provide your agent (AI) with the opportunity to learn by trial and error.<\/p>\n\n\n\n<p>This approach is specifically aimed at finding the best solution to a problem. Iterative problem solving, resource optimization, and scenario simulation are examples of this pattern. Reinforcement learning approaches to machine learning dominate some of the most popular examples and use cases of this pattern.<\/p>\n\n\n\n<p>Reinforcement learning is the process of learning to map from conditions to behavior to optimize a scalar gain or a reinforcing alert. The learning system is not told what action to take, as with other ways of learning. Instead, it must figure out which actions produce the highest reward by attempting them.<\/p>\n\n\n\n<p>For the most fascinating and difficult situations, actions influence not just the instant reward but also the scenario that follows, and all future incentives arising from that. The trial-and-error search and delayed incentive are the two defining characteristics of reinforcement learning. Let\u2019s take a deeper look into reinforcement learning and how it enables goal-driven systems.<\/p>\n\n\n\n<h3>The Reinforcement Learning Secrets<\/h3>\n\n\n\n<p>Reinforcement learning is probably the hardest field of ML to grasp because you will find that there are so many things happening simultaneously or at the same time.<\/p>\n\n\n\n<p>Chess is a prime example of how the RL algorithm works. The software knows the game rules and how to playand will go through the steps to finish the round. The only piece of information the program gets is whether it has won or lost the match. It continues playing the game until it eventually wins a match, all while keeping track of its positive moves. &nbsp;This was a non-technical way of explaining how reinforcement learning works. Let\u2019s now take a look at a more technical explanation of RL.<\/p>\n\n\n\n<h3>Markov Decision Process<\/h3>\n\n\n\n<p>A cycle of trial and error, reinforcement learning involves the agent taking a series of actions in a setting. The agent has a state every single moment, and acts from that specific state to a new one. There may or may not be a reward for this particular behavior.<\/p>\n\n\n\n<p>So, we can conclude that any epoch of learning (or an episode) can be interpreted as a series of states, acts, and incentives. Each state depends only on the preceding states and behavior and is essentially stochastic as the setting. The Markov property is satisfied by this process.<\/p>\n\n\n\n<p>The Markov property implies that the conditional probability distribution of process states in the future is dependent only on the current state and not on the series of events that came before it. This entire process is called the Markov decision process.<\/p>\n\n\n\n<h2>The Categories of Reinforcement Learning<\/h2>\n\n\n\n<p>Today, there are many different types of reinforcement learning algorithms. Each RL algorithms focuses on something different. While there are many unique RL algorithms, model-based and model-free are the two main categories that reinforcement learning algorithms can be classified into. The following is a brief explanation of each category.<\/p>\n\n\n\n<h3>1. Model-Based<\/h3>\n\n\n\n<p>These algorithms are designed to learn from their observations how the world (its mechanics) functions, and then prepare a solution by utilizing that model. Once they have a blueprint, they use some form of preparation to find the best strategy. They are known to be efficient in data. However, they struggle when space is too big for the state. &nbsp;Examples of model-based approaches are dynamic programming methods. This is because they need maximum environmental awareness, such as probabilities of change and incentives.<\/p>\n\n\n\n<h3>2. Model-Free<\/h3>\n\n\n\n<p>Model-free algorithms do not need to study the environment and preserve all states and behavior combinations. They can be classified into the following two groups, depending upon the training&#8217;s ultimate objective.<\/p>\n\n\n\n<h4>Policy-Based Methods<\/h4>\n\n\n\n<p>They look to find the best strategy, be it deterministic or stochastic. In this group, you will find algorithms such as REINFORCE and policy gradients. The benefits of these algorithms are increased coordination and efficacy in high dimensional or dynamic spaces of operation.<\/p>\n\n\n\n<p>Policy-based approaches are an issue of optimization, where we consider a policy function to its limit. This is why we also use algorithms such as Hill Climbing and Evolution Strategies (ESs).<\/p>\n\n\n\n<h4>Value-Based Methods<\/h4>\n\n\n\n<p>They look to determine the optimum value. A significant part of this group is an algorithm family called Q-learning, which learns to maximize the Q-value. Q-learning is an important part of Reinforcement learning. Other algorithms include Value Iteration and SARSA.<\/p>\n\n\n\n<p>At the convergence of policy and value-based approaches, we have the Actor-Critical approaches, which seek to maximize both policy and value function.<\/p>\n\n\n\n<h2>Understanding Deep Reinforcement Learning<\/h2>\n\n\n\n<p>Deep neural networks have been utilized to model environmental dynamics (mode-based), boost policy searches (policy-based), and estimate the value function (value-based). A model called Deep Q Network has resulted from research on the value function. This is a model credited for some of the most impressive developments in the field, along with its many upgrades (e.g. Atari).<\/p>\n\n\n\n<p>Deep reinforcement learning incorporates artificial neural networks into a reinforcement learning system that allows software-defined agents to recognize the optimal behaviors in a simulated environment to achieve their objectives. That is to say it binds function estimation and optimization of targets, connecting pairs of state-action to anticipated incentives. &nbsp;Is this too complicated for you to understand? Here\u2019s deep reinforcement learning explained in a more layman language.<\/p>\n\n\n\n<p>Deep reinforcement learning is a subset of machine learning and AI where advanced systems can learn from their behaviors in a manner similar to how humans learn from experience. Intrinsic in this form of machine learning is the rewarding or penalizing of an agent based on their behaviors. Actions are rewarded (reinforced) if they get them to the desired result.<\/p>\n\n\n\n<p>A machine continues to learn through a sequence of trials and errors, making this technology perfect for dynamic environments that change continuously. While reinforcement learning has existed for many years, it has been combined with deep learning only recently hand and this has produced remarkable results.<\/p>\n\n\n\n<p>The &#8220;deep&#8221; part of reinforcement learning indicates several (deep) layers of artificial neural networks that mimic the design of a human brain. Deep learning demands a lot of computing resources and large volumes of training data. However, data volumes have grown exponentially in the past few years, while the cost of computing power has fallen significantly, allowing for the proliferation of deep learning applications.<\/p>\n\n\n\n<p>After the highly-publicized loss of a Go grandmaster at the hands of <a href=\"https:\/\/deepmind.com\/research\/case-studies\/alphago-the-story-so-far\">DeepMind&#8217;s AlphaGo<\/a>, the prospects of deep reinforcement learning were brought to the attention of many. Deep reinforcement learning does not excel at playing Go, but it has also reached human-level mastery in other games as well. These games include Atari, poker, chess, and other video\/online games.&nbsp; The applications of deep reinforcement learning are not limited to games. Their practical applications can be found in the following sectors as well.<\/p>\n\n\n\n<h3>Training<\/h3>\n\n\n\n<p>AI toolkits like Psychlab, DeepMind Lab, and OpenAI Gym provide the training environment required to propel large-scale development for in-depth reinforcement learning. Open source tools, they are designed to train DRL agents. We will continue to see rapid growth in practical implementations as more companies apply deep reinforcement learning to their own specific business use cases.<\/p>\n\n\n\n<h3>Automotive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/h3>\n\n\n\n<p>The automotive industry has a varied and extensive dataset that will allow deep learning to be strengthened. It will help optimize manufacturing, vehicle servicing and overall automation in industry; it is already in use for autonomous vehicles. The automotive industry is defined by reliability, cost and quality, and DRL can provide new opportunities to increase quality, save costs and ensure a better safety record using data from consumers, suppliers and warranties.<\/p>\n\n\n\n<h3>Healthcare<\/h3>\n\n\n\n<p>From diagnosis and assessing appropriate treatment plans to clinical trials, development of new drug, and automated treatment, there is tremendous potential for developing healthcare through deep reinforcement learning.<\/p>\n\n\n\n<h3>Manufacturing<\/h3>\n\n\n\n<p>Intelligent robots in warehouse and distribution centers are becoming increasingly common for sorting out millions of goods and distributing them to the right people. When a robot chooses a unit to place in a container, deep reinforcement learning allows it acquire knowledge based on whether it has succeeded or failed. The robot then uses this knowledge to perform better in the future.<\/p>\n\n\n\n<p>A real-world example of this is BRETT, previously known Willow Garage Personal Robot 2 (PR2). BRETT is a <a href=\"https:\/\/news.berkeley.edu\/2015\/05\/21\/deep-learning-robot-masters-skills-via-trial-and-error\/\">robot developed by a team of researchers at UC Berkeley\u2019s Department of Electrical Engineering and Computer Sciences worked with<\/a>.<\/p>\n\n\n\n<p>The algorithm regulating the learning of the robot included a component of reward, which generated a score based on how well the robot was performing the job. &nbsp;Movements that took the robot closer to accomplishing the job will result in higher scores than those that do not. The score feeds back via the neural net in order to allow the robot to learn which moves are better for the job.<\/p>\n\n\n\n<h3>Conversational Bots<\/h3>\n\n\n\n<p>The conversational User interface framework that enables AI bots utilizes the power of deep reinforcement learning. Thanks to deep reinforcement learning, the bots are rapidly mastering the complexities and terminology of language in several domains for automated speech and natural language comprehension.<\/p>\n\n\n\n<h2>3 Ways Machine Learning and Other Cognitive Approaches Are Helping Find the Optimal Solution through Iterations of Trial And Error<\/h2>\n\n\n\n<p>The following are some of the main ways machine learning and other cognitive approaches are helping find the optimal solution through iterations of trial and error.<\/p>\n\n\n\n<h3>1. Scenario Simulation<\/h3>\n\n\n\n<p>A scenario simulation is an artificial illustration of a real-world occurrence by means of experiential learning to accomplish educational objectives. It takes careful preparation to develop a successful simulation scenario and it can be divided into several phases.<\/p>\n\n\n\n<p>Simulation scenarios are developed to evaluate, inform and assist learners in finding differences in their comprehension of the content or information application. Simulation may also be used to address patient health concerns, clinical events, communication difficulties, organizational skills, the potential for teamwork, and leadership skills. A real-world example of scenario simulation is<a href=\"https:\/\/techcrunch.com\/2017\/08\/10\/phishme-releases-free-phishing-training-tools-for-smbs\/\"> PhishMe<\/a>.<\/p>\n\n\n\n<p>Hackers are constantly using spearphishing and phishing email attacks to target employees. To counter this danger, security firm PhishMe has built simulation software that can be used by a business executive, human resource, IT, and auditors to inform their employees.<\/p>\n\n\n\n<p>PhishMe assists in generating and distributing focused phishing messages with hyperlinks, including those requesting username and password, or attachments intended to engage users. The platform then collects user response metrics in a centralized storage location for organizations to create reports or share them with users.<\/p>\n\n\n\n<h3>2. Resource Optimization<\/h3>\n\n\n\n<p>Resource optimization helps organizations to satisfy resource requests optimally. Resource optimization analyses all open resource requests against available resources and sets out a strategy that helps you to meet the objectives of the company.<\/p>\n\n\n\n<p>The AI, resource optimization tool, utilizes a range of sophisticated AI techniques, like genetic algorithms, business rules, and constraint programming, to intelligently generate tailored resource allocation timelines and strategies that reliably fulfill goal requirements while fulfilling all rules and constraints. Using an advanced dynamic rescheduling algorithm, this AI tool also enables organizations to respond to unforeseen events or changes in workload demands.<\/p>\n\n\n\n<h3>3. Iterative Problem Solving<\/h3>\n\n\n\n<p>Iterative problem solving in AI and machine learning takes inspiration from the software-development lifecycle, which is essentially a process in which you are solving a problem\u2014in well-defined steps. Like software development, problem-solving in machine learning and AI is an iterative process which includes the following steps:<\/p>\n\n\n\n<ol type=\"1\"><li>Recognizing the Problem<\/li><li>Reviewing the Dataset<\/li><li>Establishing a Realistic End Goal<\/li><li>Listing Alternate Solutions<\/li><li>Selecting a Solution<\/li><li>Implementing the solutions<\/li><li>Evaluation<\/li><\/ol>\n\n\n\n<p>By going through the above steps, machine learning algorithms can understand a problem and then solve it iteratively.<\/p>\n\n\n\n<h2>Final Word<\/h2>\n\n\n\n<p>Machines have mastered games of such as chess and checkers and quickly found answers to mazes in the past. Now, they can compete at Go, multi-player games like DoTA, and even more complex games, thanks to the power of reinforcement learning and other sophisticated computing capabilities.<\/p>\n\n\n\n<p>For goal-driven systems, games are not the only area of application. Machine learning and other cognitive methods can be implemented by companies to leverage the power of reinforcement learning and other machine learning strategies to equip their applications with the ability to learn by trial and error. It is valuable in any situation where you want the application to find the best or optimal solution to a problem. COVID-19 vaccine development is a latest and greatest example of this process.<\/p>\n\n\n\n<p>In the final article in this series, we will look at how a combination of Smart Workflow, RPA and other semantic automation methods can be used to simplify everyday routine tasks and free human resources for more creative tasks.<\/p>\n","protected":false},"excerpt":{"rendered":"","protected":false},"author":2,"featured_media":3334,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[1],"tags":[85],"_links":{"self":[{"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/posts\/3310"}],"collection":[{"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/users\/2"}],"replies":[{"embeddable":true,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/comments?post=3310"}],"version-history":[{"count":3,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/posts\/3310\/revisions"}],"predecessor-version":[{"id":3336,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/posts\/3310\/revisions\/3336"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/media\/3334"}],"wp:attachment":[{"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/media?parent=3310"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/categories?post=3310"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/tags?post=3310"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}