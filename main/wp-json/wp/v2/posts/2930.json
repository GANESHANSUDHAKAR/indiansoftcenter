{"id":2930,"date":"2019-09-24T15:02:45","date_gmt":"2019-09-24T19:02:45","guid":{"rendered":"https:\/\/achievion.com\/?p=2930"},"modified":"2019-09-24T15:02:47","modified_gmt":"2019-09-24T19:02:47","slug":"ethical-ai-101-ensuring-an-unbiased-ai","status":"publish","type":"post","link":"https:\/\/achievion.com\/blog\/ethical-ai-101-ensuring-an-unbiased-ai.html","title":{"rendered":"Ethical AI 101: Ensuring an Unbiased AI"},"content":{"rendered":"\n<p>This post is the third entry in our series on the Ethical AI principles.<\/p>\n\n\n\n<p>In the <a href=\"https:\/\/achievion.com\/blog\/understanding-the-first-3-principles-for-ethical-ai.html\">previous post<\/a>, we discussed the first principle of the Ethical AI concept. In this post, we will discuss another principle of Ethical AI which refers to \u2018ensuring an unbiased AI.\u2019&nbsp;<\/p>\n\n\n\n<p>We will start with a discussion on the bias in AI problem before moving onto the reasons for AI bias, the greatest challenges in overcoming it, and how to tackle the problem. Let\u2019s begin.<\/p>\n\n\n\n<h2><strong>Understanding the Bias in AI Problem<\/strong><strong><\/strong><\/h2>\n\n\n\n<p>Before we start the discussion in the bias in AI problem, let\u2019s quickly recap what we learned about \u2018ensuring an unbiased AI\u2019 in the <a href=\"https:\/\/achievion.com\/blog\/understanding-the-ethical-ai-and-explainable-ai-concepts.html\">first post<\/a>&nbsp;of this series of articles relating to the Ethical AI concept.<\/p>\n\n\n\n<p>In the first post in this series, we learned that the main purpose of AI was eliminating bias in processes. Additionally, we learned that an AI system must be controlled for negative or harmful human bias during development and maintenance. This is to eliminate any kind of bias\u2014whether it is related to age, race, gender, sexual orientation,&nbsp;etc.\u2014in operation of the AI system.<\/p>\n\n\n\n<p>It is critical for designers and developers of AI system to consider the dimensions of diversity relating to culture, humans, and systems from the very beginning of the process. Failure to do so may lead to the creation of AI systems with a default mode that is irrelevant to excluded groups; often,&nbsp;this is where the problem of discrimination or bias in AI starts.<\/p>\n\n\n\n<p>When the concept of AI decision making was first introduced, many people believed that it would solve the problem of bias in society which had long gone unresolved. It was thought that computers won\u2019t be affected by bias since they have no inherent views on things like age, race, gender, and sexual orientation.<\/p>\n\n\n\n<p>This was true back in the day as computers had&nbsp;limited functionality. However, this changed with the rollout of machine learning. The Big Data explosion and a decrease in the costs of computing&nbsp;with adequate processing power that can handle it posed a new challenge.<\/p>\n\n\n\n<p>In the past, the importance of high-quality data was succinctly summed up by the term \u2018garbage in, garbage out\u2019. This meant that if you provided computers with poor data, the results returned by them wouldn\u2019t be very helpful or favorable.<\/p>\n\n\n\n<p>In the early days of AI, this was a problem only for computer programmers and analysts. However, today it\u2019s a problem for everybody since computers today are often asked to make decisions regarding inviting applicants to job interviews, someone\u2019s eligibility for a mortgage, and other important things that affect\u00a0lives.<\/p>\n\n\n\n<p>Perhaps, the biggest example of <a href=\"https:\/\/www.propublica.org\/article\/how-we-analyzed-the-compas-recidivism-algorithm\">bias in AI<\/a>&nbsp;is the use of an algorithm that was biased against African-Americans by US parole authorities to predict the probability of criminals reoffending. This clearly shows that&nbsp;bias in AI exists. This problem must be tackled and resolved before it starts to have a lasting impact on society.<\/p>\n\n\n\n<h2><strong>Why It Happens<\/strong><strong><\/strong><\/h2>\n\n\n\n<p>Today, most AI applications are based on deep learning, which is a category of AI algorithms. Another thing enabling these apps is the way deep-learning algorithms find patterns in data. Therefore, deep-learning algorithms can affect the lives of people and prolong bias in recruitment, security, retail,&nbsp;and other things.<\/p>\n\n\n\n<p>So, we are aware that bias in AI exists but how does this problem arise in the first place? Many people think that it is due to bias in training data. While biased training data is one of the reasons for AI bias, it is only a part of the problem rather than the problem itself. In fact, the bias in AI can creep in long before any data collection and dissemination happens. While the problem of bias in AI occurs at many stages of the deep-learning process, the following three stages are where you need to pay the most attention.<\/p>\n\n\n\n<h3><strong>1.&nbsp;<\/strong><strong>Problem Framing<\/strong><strong><\/strong><\/h3>\n\n\n\n<p>When creating a deep learning model, the very first decision that computer scientists make is what they want to achieve. For example, a credit card company may want to create a deep-learning model to predict the creditworthiness of a customer. The only problem here is that \u2018creditworthiness\u2019 is a vague or unclear concept.<\/p>\n\n\n\n<p>To convert \u2018creditworthiness\u2019 into a computable factor, the company must make a decision: does it want to maximize the number of loans to be repaid or does it want to maximize its profits margins. Once a decision is made, the company can define creditworthiness in the context of the selected goal.<\/p>\n\n\n\n<p>However, here is where the problem of AI bias creeps in; say, the company chose to maximize its profits margins. Now, while this isn\u2019t the company\u2019s intention, the deep-learning algorithm may engage in predatory behavior to give out loans in order to maximize profits. This may lead to bias or discrimination in decisions about giving out loans even though the company did not intend it.<\/p>\n\n\n\n<h3><strong>2.&nbsp;Data Collection<\/strong><\/h3>\n\n\n\n<p>In training data, bias shows up in two different ways; the data collected either reflects existing biases or does not represent reality. For example, the latter may occur if you feed a deep-learning algorithm with more images of light-skinned faces than dark-skinned faces, it is inevitable that the face recognition system that results from this will be <a href=\"https:\/\/www.technologyreview.com\/s\/612846\/making-face-recognition-less-biased-doesnt-make-it-less-scary\/\">biased&nbsp;towards light-skinned faces<\/a>&nbsp;since recognizing darker-skinned faces would be difficult for it.<\/p>\n\n\n\n<p>The other way in which bias can show up in data collection can be explained with what happened with Amazon. The e-commerce giant recently found that its <a href=\"https:\/\/www.reuters.com\/article\/us-amazon-com-jobs-automation-insight\/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\">internal recruiting tool<\/a>&nbsp;had been dismissing&nbsp;female candidates. On investigation, the company found that this bias in recruiting was because the AI system was trained on past hiring decisions which favored men over women.<\/p>\n\n\n\n<h3>3<strong>.&nbsp;Data Preparation<\/strong><\/h3>\n\n\n\n<p>Another stage of the deep-learning process at which bias in AI occurs is data preparation. This is the stage at which you choose the attributes which you want the deep learning algorithm to consider. Although it may appear as such, data preparation is very different from problem framing. <\/p>\n\n\n\n<p>In data preparation, you can use different attributes to train a model for the same goal or use similar attributes to train a model for varying goals. For example, if you were to model \u2018creditworthiness\u2019, then the age or income of the customer could be an \u2018attribute\u2019. <\/p>\n\n\n\n<p>On the other hand, the gender, years of experience, or education level of the candidate could be an \u2018attribute\u2019 in case of Amazon\u2019s recruiting tool. Often, this is referred to as the \u2018art\u2019 of deep learning; the prediction accuracy of your model can be significantly affected by the attributes you choose to consider or ignore. &nbsp;While measuring the impact of this&nbsp;model\u2019s accuracy is easy, it can be difficult to determine the impact on the model\u2019s bias.<\/p>\n\n\n\n<h2><strong>The Biggest Challenges in Fixing AI Bias<\/strong><strong><\/strong><\/h2>\n\n\n\n<p>Now that you understand the problem of bias in AI and why it happens, you may want to fix the problem. However, solving the problem of AI bias is easier said than done. This is due to&nbsp;the following challenges that lie in the way of this task:<\/p>\n\n\n\n<h3><strong>1.&nbsp;AI Bias isn\u2019t Obvious Until Much Later in the Deep-Learning Process<\/strong><\/h3>\n\n\n\n<p>One of the greatest challenges in fixing AI bias is that bias in a deep-learning model isn\u2019t obvious from the start. It is much later in the process that you start to realize the impact of your data and choices. At this point, it may be difficult to pinpoint where the bias originated and then find a way to eliminate it.<\/p>\n\n\n\n<p>In the case of Amazon, engineers reprogrammed the recruiting tool after discovering bias in it. They re-trained the tool to ignore explicit gender-specific words like \u201cwomen\u2019s\u201d. However, this did not solve the problem completely,&nbsp;as the AI tool continued to pick up on implicit gendered words that often resulted in it choosing \u2018men over women\u2019.<\/p>\n\n\n\n<h3>2<strong>.&nbsp;Inappropriately Designed Processes<\/strong><\/h3>\n\n\n\n<p>Another challenge in fixing AI bias is inappropriately designed or imperfect processes; many deep learning practices are designed without considering bias detection. Before deployment, deep-learning models are checked for performance. In theory, this would be the perfect opportunity to catch bias. However, there is no practical application of this.<\/p>\n\n\n\n<p>Instead, data is split randomly by the developers\/programmers and then trained. While one group is used for training, the other is reserved for validation after the completion of training. This means that the data used for checking the model\u2019s performance has the same biases as the data used for training; this results&nbsp;in a failure to flag prejudiced results.<\/p>\n\n\n\n<p>In addition to the above, some other challenges in fixing the problem of bias in AI are the lack of social context in framing problems for the deep-learning model and a failure to define the concept of fairness in mathematical terms.<\/p>\n\n\n\n<h2><strong>How to Tackle the Problem of Bias in AI<\/strong><strong><\/strong><\/h2>\n\n\n\n<p>Considering the challenges in fixing AI bias outlined in the previous section, achieving a bias-free deep learning system may be seen unattainable. However, that is not the case. While enabling a bias-free deep learning system can be a real challenge, it is not out of reach. Following are some of the ways to tackle the problem of bias in AI to enable bias-free deep learning models and AI systems.<\/p>\n\n\n\n<h3><strong>1.&nbsp;<\/strong><strong>Define Attributes to Create Guidelines<\/strong><strong><\/strong><\/h3>\n\n\n\n<p>One of the most important steps in solving the problem of bias in AI is defining attributes to create guidelines for the deep learning model. For example, if the deep-learning model is a system that answers&nbsp;a simple question such as \u2018what is a human,\u2019 then you need to define what human means here. <\/p>\n\n\n\n<p>You need to be clear about what it includes and what it doesn\u2019t. &nbsp;You need to identify the problem you\u2019re solving and everything that could go wrong. Basically, you should know what you\u2019re building and what problems the end-user could likely face. Once you have this information,&nbsp;start a deep review&nbsp;of&nbsp;your data. You must review your data for possible underrepresentation or bias in results; you need to think about all the issues related to bias that could arise in the deep-learning process. Some of the questions to ask yourself during the data review in order to identify and eliminate bias are:<\/p>\n\n\n\n<ul><li>What is the source of the data? Is this source subject to bias?<\/li><li>Am I thinking enough about the end-user?<\/li><\/ul>\n\n\n\n<p>By answering the above, you will be able to carefully define your problem and end-users. Additionally, you will be able to plan for your outcomes and solve many potential issues by paying careful attention to your training data.<\/p>\n\n\n\n<h3><strong>2.\u00a0Avoid Training AI on Data Containing Unfair Outcomes<\/strong><\/h3>\n\n\n\n<p>AI systems do only what they\u2019re taught; AI needs to be trained using historical data or examples. However, the use of biased data when training AI should be avoided; if this does not happen, then the decisions made by AI are likely to be biased.<\/p>\n\n\n\n<p>If you were a high school teacher, would you teach your student using textbooks that were published more than a century ago? No, you wouldn\u2019t! This is the exact thinking you need when training AIs. If you train AI on poor or biased data, then the outcomes will also be biased or poor. <\/p>\n\n\n\n<p>For example, if the data used to train a deep learning model was from a time when&nbsp;men were likely to be hired than woman, then the AI will learn to prioritize men over women when making hiring decisions. Therefore, you need to make sure that the data you use to train an AI does not contain any unfair outcomes.<\/p>\n\n\n\n<h3><strong>3.\u00a0Be Careful with Your Use of Data<\/strong><\/h3>\n\n\n\n<p>Another advice for tackling the problem of bias in AI is being careful with your use of data. Some of the best practices for this are:<\/p>\n\n\n\n<ul><li>Being transparent and open about the data used to train the system <\/li><li>Making clear the criteria and purpose of the decision making<\/li><li>Acknowledging that the AI system will have different end-users who will use the system differently<\/li><li>Taking feedback<\/li><li>Ensuring the deep learning\/AI model actively learns from real-world data and new examples<\/li><\/ul>\n\n\n\n<h2><strong>Final Word<\/strong><strong><\/strong><\/h2>\n\n\n\n<p>Eliminating bias from AI is not only an Ethical AI requirement, but it also a key factor in the success of an AI system. In this post, we discussed the AI bias problem including what it entails, why it happens, the challenges in fixing it, and how to tackle the problem.<\/p>\n\n\n\n<p>In the next article in this series, we will look at the next principle of Ethical AI as it is one of the keys to fulfilling the requirement of the Ethical AI concept.<\/p>\n","protected":false},"excerpt":{"rendered":"","protected":false},"author":2,"featured_media":2935,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[1],"tags":[85],"_links":{"self":[{"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/posts\/2930"}],"collection":[{"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/users\/2"}],"replies":[{"embeddable":true,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/comments?post=2930"}],"version-history":[{"count":5,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/posts\/2930\/revisions"}],"predecessor-version":[{"id":2938,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/posts\/2930\/revisions\/2938"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/media\/2935"}],"wp:attachment":[{"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/media?parent=2930"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/categories?post=2930"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/achievion.com\/wp-json\/wp\/v2\/tags?post=2930"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}